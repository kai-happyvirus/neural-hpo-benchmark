{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d397c2",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms for Hyperparameter Optimization in Neural Networks\n",
    "\n",
    "This notebook contains runnable code for MNIST (FNN) and CIFAR-10 (CNN) experiments with:\n",
    "- Random Search\n",
    "- Genetic Algorithm (GA via DEAP)\n",
    "- Differential Evolution (DE)\n",
    "- Particle Swarm Optimization (PSO)\n",
    "\n",
    "Use small settings first to verify execution, then scale up for the report.\n",
    "\n",
    "**Dependencies**: `torch`, `torchvision`, `deap`, `numpy`, `pandas`, `matplotlib`, `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764b554",
   "metadata": {},
   "source": [
    "## 1. Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e36501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, json, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33545112",
   "metadata": {},
   "source": [
    "## 2. Data Loading (MNIST & CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef091a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MNIST transforms\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# CIFAR-10 transforms\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Download datasets (first run fetches them)\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "mnist_test  = datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
    "\n",
    "cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar_transform)\n",
    "cifar_test  = datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "len(mnist_train), len(mnist_test), len(cifar_train), len(cifar_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c35ef9",
   "metadata": {},
   "source": [
    "## 3. Models (FNN for MNIST, CNN for CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, hidden_layers=[128, 64], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = 28*28\n",
    "        last = input_dim\n",
    "        for h in hidden_layers:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        layers += [nn.Linear(last, 10)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, channels=[32, 64], dropout=0.25, fc=128):\n",
    "        super().__init__()\n",
    "        c1, c2 = channels\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, c1, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(c1, c2, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c2*16*16, fc), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(fc, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe4e52",
   "metadata": {},
   "source": [
    "## 4. Training & Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bb265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        pred = out.argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        pred = out.argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return total_loss/total, correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f3dca",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Search Space & Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR_BOUNDS = (1e-4, 1e-1)\n",
    "BATCH_CHOICES = [32, 64, 128]\n",
    "DROPOUT_BOUNDS = (0.1, 0.5)\n",
    "HIDDEN_LAYER_CHOICES = [\n",
    "    [128],\n",
    "    [256],\n",
    "    [256, 128],\n",
    "    [512, 256]\n",
    "]\n",
    "OPTIMIZERS = ['sgd', 'adam', 'rmsprop']\n",
    "\n",
    "def decode(ind):\n",
    "    lr = 10 ** (np.log10(LR_BOUNDS[0]) + ind[0]*(np.log10(LR_BOUNDS[1])-np.log10(LR_BOUNDS[0])))\n",
    "    batch = BATCH_CHOICES[int(ind[1]*len(BATCH_CHOICES)) % len(BATCH_CHOICES)]\n",
    "    dropout = DROPOUT_BOUNDS[0] + ind[2]*(DROPOUT_BOUNDS[1]-DROPOUT_BOUNDS[0])\n",
    "    hidden = HIDDEN_LAYER_CHOICES[int(ind[3]*len(HIDDEN_LAYER_CHOICES)) % len(HIDDEN_LAYER_CHOICES)]\n",
    "    opt = OPTIMIZERS[int(ind[4]*len(OPTIMIZERS)) % len(OPTIMIZERS)]\n",
    "    return {'lr': float(lr), 'batch': int(batch), 'dropout': float(dropout),\n",
    "            'hidden': hidden, 'opt': opt}\n",
    "\n",
    "def get_optimizer(name, params, lr):\n",
    "    if name == 'sgd': return optim.SGD(params, lr=lr, momentum=0.9)\n",
    "    if name == 'adam': return optim.Adam(params, lr=lr)\n",
    "    if name == 'rmsprop': return optim.RMSprop(params, lr=lr, momentum=0.9)\n",
    "    raise ValueError(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2838f",
   "metadata": {},
   "source": [
    "## 6. Fitness Functions (MNIST/FNN and CIFAR-10/CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fitness_mnist(ind, epochs=1):\n",
    "    hp = decode(ind)\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(mnist_train, batch_size=hp['batch'], shuffle=True)\n",
    "    val_loader   = DataLoader(mnist_test,  batch_size=256)\n",
    "    # Model + optimizer\n",
    "    model = FNN(hidden_layers=hp['hidden'], dropout=hp['dropout']).to(DEVICE)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = get_optimizer(hp['opt'], model.parameters(), hp['lr'])\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, crit, opt)\n",
    "    vloss, vacc = evaluate(model, val_loader, crit)\n",
    "    return (vacc,)\n",
    "\n",
    "def fitness_cifar(ind, epochs=1):\n",
    "    hp = decode(ind)\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(cifar_train, batch_size=hp['batch'], shuffle=True)\n",
    "    val_loader   = DataLoader(cifar_test,  batch_size=256)\n",
    "    # Model + optimizer (CNN)\n",
    "    model = SimpleCNN(channels=[32,64], dropout=hp['dropout'], fc=128).to(DEVICE)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = get_optimizer(hp['opt'], model.parameters(), hp['lr'])\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, crit, opt)\n",
    "    vloss, vacc = evaluate(model, val_loader, crit)\n",
    "    return (vacc,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d2eec",
   "metadata": {},
   "source": [
    "## 7. Baseline: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b416c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_search(fitness_fn, trials=5):\n",
    "    best = (-1, None)\n",
    "    hist = []\n",
    "    for t in range(trials):\n",
    "        ind = [random.random() for _ in range(5)]\n",
    "        acc = fitness_fn(ind)[0]\n",
    "        hist.append(acc)\n",
    "        if acc > best[0]:\n",
    "            best = (acc, decode(ind))\n",
    "        print(f\"Trial {t+1}/{trials} -> acc={acc:.4f}\")\n",
    "    return best, hist\n",
    "\n",
    "# Example smoke tests (uncomment to run quickly)\n",
    "# best_rs_mnist, hist_rs_mnist = random_search(fitness_mnist, trials=2)\n",
    "# best_rs_cifar, hist_rs_cifar = random_search(fitness_cifar, trials=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36244f",
   "metadata": {},
   "source": [
    "## 8. Genetic Algorithm (DEAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register DEAP structures once (avoid duplicate creator definitions when re-running the cell)\n",
    "try:\n",
    "    creator.FitnessMax\n",
    "except Exception:\n",
    "    creator.create('FitnessMax', base.Fitness, weights=(1.0,))\n",
    "    creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "def run_ga(fitness_fn, pop_size=8, gens=3, cxpb=0.5, mutpb=0.3):\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register('attr_float', random.random)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.attr_float, 5)\n",
    "    toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register('evaluate', fitness_fn)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb=0.5)\n",
    "    toolbox.register('mutate', tools.mutGaussian, mu=0.0, sigma=0.1, indpb=0.5)\n",
    "    toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "    hof = tools.HallOfFame(1)\n",
    "\n",
    "    # Track per-generation best for convergence plots\n",
    "    per_gen_best = []\n",
    "\n",
    "    def record_stats(pop):\n",
    "        best = max(pop, key=lambda ind: ind.fitness.values[0])\n",
    "        per_gen_best.append(best.fitness.values[0])\n",
    "\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=gens, halloffame=hof, verbose=False)\n",
    "    # After eaSimple, evaluate per generation isn't recorded by default; re-run a manual loop if needed.\n",
    "    # For simplicity, we append only final best here; users can extend to log per-gen.\n",
    "    if not per_gen_best:\n",
    "        per_gen_best.append(hof[0].fitness.values[0])\n",
    "\n",
    "    return hof[0].fitness.values[0], decode(hof[0]), per_gen_best\n",
    "\n",
    "# Example smoke tests (uncomment to run quickly)\n",
    "# acc_ga_mnist, hp_ga_mnist, log_ga_mnist = run_ga(fitness_mnist, pop_size=6, gens=2)\n",
    "# acc_ga_cifar, hp_ga_cifar, log_ga_cifar = run_ga(fitness_cifar, pop_size=6, gens=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f3050",
   "metadata": {},
   "source": [
    "## 9. Differential Evolution (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f97682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clamp01(x): return max(0.0, min(1.0, x))\n",
    "\n",
    "def run_de(fitness_fn, pop_size=8, gens=3, F=0.5, CR=0.9):\n",
    "    pop = [np.random.rand(5) for _ in range(pop_size)]\n",
    "    fit = [fitness_fn(ind.tolist())[0] for ind in pop]\n",
    "    per_gen_best = [max(fit)]\n",
    "    for _ in range(gens):\n",
    "        for i in range(pop_size):\n",
    "            a, b, c = np.random.choice([j for j in range(pop_size) if j != i], 3, replace=False)\n",
    "            mutant = pop[a] + F*(pop[b]-pop[c])\n",
    "            trial = np.array([mutant[j] if random.random() < CR else pop[i][j] for j in range(5)])\n",
    "            trial = np.array([clamp01(v) for v in trial])\n",
    "            f_trial = fitness_fn(trial.tolist())[0]\n",
    "            if f_trial > fit[i]:\n",
    "                pop[i], fit[i] = trial, f_trial\n",
    "        per_gen_best.append(max(fit))\n",
    "    bi = int(np.argmax(fit))\n",
    "    return fit[bi], decode(pop[bi].tolist()), per_gen_best\n",
    "\n",
    "# Example smoke tests (uncomment to run quickly)\n",
    "# acc_de_mnist, hp_de_mnist, log_de_mnist = run_de(fitness_mnist, pop_size=6, gens=2)\n",
    "# acc_de_cifar, hp_de_cifar, log_de_cifar = run_de(fitness_cifar, pop_size=6, gens=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f10a2",
   "metadata": {},
   "source": [
    "## 10. Particle Swarm Optimization (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pso(fitness_fn, n_particles=8, iters=3, w=0.7, c1=1.5, c2=1.5):\n",
    "    dim = 5\n",
    "    X = np.random.rand(n_particles, dim)\n",
    "    V = np.zeros((n_particles, dim))\n",
    "    pbest = X.copy()\n",
    "    pfit = np.array([fitness_fn(x.tolist())[0] for x in X])\n",
    "    gi = int(np.argmax(pfit))\n",
    "    gbest = pbest[gi].copy()\n",
    "    gfit = pfit[gi]\n",
    "    per_iter_best = [gfit]\n",
    "\n",
    "    for _ in range(iters):\n",
    "        for i in range(n_particles):\n",
    "            r1, r2 = np.random.rand(dim), np.random.rand(dim)\n",
    "            V[i] = w*V[i] + c1*r1*(pbest[i]-X[i]) + c2*r2*(gbest-X[i])\n",
    "            X[i] = np.clip(X[i] + V[i], 0.0, 1.0)\n",
    "            f = fitness_fn(X[i].tolist())[0]\n",
    "            if f > pfit[i]:\n",
    "                pfit[i], pbest[i] = f, X[i].copy()\n",
    "                if f > gfit:\n",
    "                    gfit, gbest = f, X[i].copy()\n",
    "        per_iter_best.append(gfit)\n",
    "    return gfit, decode(gbest.tolist()), per_iter_best\n",
    "\n",
    "# Example smoke tests (uncomment to run quickly)\n",
    "# acc_pso_mnist, hp_pso_mnist, log_pso_mnist = run_pso(fitness_mnist, n_particles=6, iters=2)\n",
    "# acc_pso_cifar, hp_pso_cifar, log_pso_cifar = run_pso(fitness_cifar, n_particles=6, iters=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04050c51",
   "metadata": {},
   "source": [
    "## 11. Orchestration: Run & Compare (start small, then scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESULTS = {}\n",
    "\n",
    "# --- MNIST quick tests ---\n",
    "# acc_ga_mn, hp_ga_mn, log_ga_mn = run_ga(fitness_mnist, pop_size=6, gens=2)\n",
    "# acc_de_mn, hp_de_mn, log_de_mn = run_de(fitness_mnist, pop_size=6, gens=2)\n",
    "# acc_pso_mn, hp_pso_mn, log_pso_mn = run_pso(fitness_mnist, n_particles=6, iters=2)\n",
    "\n",
    "# RESULTS['MNIST'] = {\n",
    "#     'GA':  {'acc': acc_ga_mn, 'hp': hp_ga_mn, 'log': log_ga_mn},\n",
    "#     'DE':  {'acc': acc_de_mn, 'hp': hp_de_mn, 'log': log_de_mn},\n",
    "#     'PSO': {'acc': acc_pso_mn, 'hp': hp_pso_mn, 'log': log_pso_mn},\n",
    "# }\n",
    "\n",
    "# --- CIFAR-10 quick tests ---\n",
    "# acc_ga_cf, hp_ga_cf, log_ga_cf = run_ga(fitness_cifar, pop_size=6, gens=2)\n",
    "# acc_de_cf, hp_de_cf, log_de_cf = run_de(fitness_cifar, pop_size=6, gens=2)\n",
    "# acc_pso_cf, hp_pso_cf, log_pso_cf = run_pso(fitness_cifar, n_particles=6, iters=2)\n",
    "\n",
    "# RESULTS['CIFAR10'] = {\n",
    "#     'GA':  {'acc': acc_ga_cf, 'hp': hp_ga_cf, 'log': log_ga_cf},\n",
    "#     'DE':  {'acc': acc_de_cf, 'hp': hp_de_cf, 'log': log_de_cf},\n",
    "#     'PSO': {'acc': acc_pso_cf, 'hp': hp_pso_cf, 'log': log_pso_cf},\n",
    "# }\n",
    "\n",
    "RESULTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8b2bc",
   "metadata": {},
   "source": [
    "## 12. Convergence Plot Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_convergence(values, title='Convergence'):\n",
    "    plt.figure()\n",
    "    plt.plot(values)\n",
    "    plt.xlabel('Generation / Iteration')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# plot_convergence(log_ga_mn, 'MNIST - GA')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
