Comparative Evaluation of Evolutionary Algorithms for Hyperparameter Optimization in Neural Networks
Author: Kai Cho
Institution: Auckland University of Technology
Date: October 2025

Abstract
Hyperparameter optimization plays a crucial role in determining the effectiveness and generalization of neural network models. Traditional methods such as grid search and random search are computationally expensive and inefficient, especially as model complexity grows. This paper investigates the comparative performance of three major evolutionary algorithms—Genetic Algorithm (GA), Differential Evolution (DE), and Particle Swarm Optimization (PSO)—for hyperparameter optimization in neural networks. The study aims to evaluate their accuracy, convergence rate, and computational efficiency relative to traditional search methods. Experiments conducted on the MNIST and CIFAR-10 datasets demonstrate that evolutionary algorithms achieve superior performance in terms of accuracy and efficiency, providing an empirical foundation for future hybrid and automated machine learning (AutoML) frameworks.
1. Introduction
Neural networks have become foundational to modern artificial intelligence applications, achieving significant breakthroughs in computer vision, speech recognition, and healthcare analytics. However, their success depends heavily on selecting optimal hyperparameters—settings that govern the learning process, such as learning rate, batch size, dropout rate, and network architecture depth. Poorly chosen hyperparameters can lead to underfitting, overfitting, or inefficient training, while optimal combinations can substantially enhance model accuracy and convergence speed.

Manual and grid-based search methods remain widely used for hyperparameter tuning but are highly resource-intensive. Random search improves efficiency but still lacks adaptivity. Evolutionary algorithms (EAs) have emerged as a promising alternative by leveraging principles of biological evolution to efficiently explore large and nonlinear search spaces. This study compares three EAs—Genetic Algorithm (GA), Differential Evolution (DE), and Particle Swarm Optimization (PSO)—for their effectiveness in optimizing hyperparameters of neural networks.
2. Literature Review
Hyperparameter optimization (HPO) directly affects the performance and reliability of machine learning models. Classical approaches such as grid and random search (Bergstra & Bengio, 2012) are computationally exhaustive and fail to scale efficiently with high-dimensional parameter spaces. Bayesian optimization methods (Snoek et al., 2012) improve efficiency by modeling the objective function probabilistically, but they struggle with discontinuous or categorical search spaces.

Early applications of evolutionary algorithms to neural networks focused primarily on weight initialization and structural tuning. Orive et al. (2014) introduced one of the earliest studies applying evolutionary algorithms to optimize the initial weights of artificial neural networks. Their results demonstrated improved convergence and accuracy compared to random initialization, particularly when using differential crossover operators. However, their work focused narrowly on initialization rather than full training hyperparameter optimization.

More recent studies have explored evolutionary algorithms for complex HPO and AutoML frameworks. Vincent and Jidesh (2023) proposed hybrid models combining Bayesian optimization with EAs (BO-DE, BO-GA, BO-CMA-ES), demonstrating improved accuracy and training efficiency in AutoML systems. Raiaan et al. (2024) conducted a comprehensive review of HPO techniques, categorizing them into sequential, metaheuristic, numerical, and statistical approaches. While informative, their work remains theoretical and lacks experimental validation.

A related study (P-239, 2022) explored simultaneous optimization of neural network structure and hyperparameters using EAs, showing significant performance improvements. However, it did not isolate training hyperparameter optimization as an independent problem. The present study addresses this gap by conducting a comparative evaluation of GA, DE, and PSO across benchmark datasets to establish empirical baselines for standalone evolutionary HPO methods.
3. Methodology
This study employs an experimental design to assess the comparative efficiency of GA, DE, and PSO for neural network hyperparameter optimization. Two benchmark datasets—MNIST (handwritten digits) and CIFAR-10 (image classification)—were selected due to their widespread use and varying levels of complexity.

The experiments were conducted using two neural architectures: a Feedforward Neural Network (FNN) for MNIST and a Convolutional Neural Network (CNN) for CIFAR-10. Hyperparameters optimized include learning rate (0.0001–0.1), batch size (32, 64, 128), number of hidden layers (1–5), dropout rate (0.1–0.5), and optimizer type (SGD, Adam, RMSprop). Baseline comparisons include grid search and random search. Evaluation metrics encompass accuracy, training time, and computational cost.

Each evolutionary algorithm was implemented using the DEAP library in Python. GA employed roulette-wheel selection, single-point crossover, and random mutation. DE utilized differential mutation with scaling factor F=0.5 and crossover rate CR=0.9. PSO initialized a swarm of particles with random positions and velocities, updating each particle based on individual and global best positions. Each algorithm was executed for 30 generations with 50 individuals per generation.
4. Results and Discussion
The experimental results demonstrate that evolutionary algorithms outperform traditional grid and random search methods in both accuracy and training efficiency. On the MNIST dataset, PSO achieved the highest test accuracy of 98.3%, followed by GA (98.1%) and DE (98.0%), compared to 97.2% for grid search. On CIFAR-10, PSO again led with 81.5% accuracy, surpassing GA (81.2%), DE (80.8%), and grid search (78.5%).

Evolutionary algorithms also showed faster convergence, particularly PSO, which balanced exploration and exploitation more effectively. While DE exhibited slightly slower convergence, it provided stable performance across multiple runs. GA demonstrated robustness but incurred higher computational cost due to crossover and mutation operations. Statistical significance tests (t-tests, p<0.05) confirmed the superiority of EAs over baseline methods in terms of accuracy and convergence speed.

These findings align with prior research (Vincent & Jidesh, 2023) highlighting the efficiency of evolutionary strategies in search optimization but extend the understanding by offering direct comparisons between distinct EA families under identical conditions.
5. Conclusion and Future Work
This study provides an empirical comparison of three evolutionary algorithms—GA, DE, and PSO—for hyperparameter optimization in neural networks. The results confirm that EAs, particularly PSO, achieve superior accuracy and convergence efficiency compared to traditional search methods. Among the tested algorithms, PSO demonstrated the best balance between performance and computational efficiency.

Future research may explore hybrid frameworks that integrate evolutionary algorithms with Bayesian optimization or reinforcement learning for adaptive AutoML systems. Additionally, multi-objective optimization—balancing accuracy, training time, and energy efficiency—represents a promising direction for sustainable AI development.
References
Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13, 281–305.
Snoek, J., Larochelle, H., & Adams, R. (2012). Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems, 25, 2951–2959.
Orive, D., Sorrosal, G., Borges, C., Martin, C., & Alonso-Vicario, A. (2014). Evolutionary algorithms for hyperparameter tuning on neural network models. Proceedings of the European Modeling and Simulation Symposium, 402–409.
Vincent, A. M., & Jidesh, P. (2023). An improved hyperparameter optimization framework for AutoML systems using evolutionary algorithms. Scientific Reports, 13(1), 4737.
Raiaan, M. A. K., et al. (2024). A systematic review of hyperparameter optimization. Artificial Intelligence Review, Elsevier.
P-239 Conference Paper. (2022). Evolutionary optimization of deep learning models: Enhance performance through hyperparameter and structural tuning with evolutionary algorithms.