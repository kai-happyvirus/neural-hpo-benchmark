{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3aa77a",
   "metadata": {},
   "source": [
    "# Evolutionary Hyperparameter Optimization Experiments\n",
    "\n",
    "This notebook implements the experimental workflow described in the project report and prepares a reproducible pipeline for benchmarking evolutionary algorithms against classical baselines on MNIST and CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323eec1d",
   "metadata": {},
   "source": [
    "## 1. Setup and Reproducibility Controls\n",
    "\n",
    "Configure the environment, ensure required packages are available, and define deterministic helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8a7786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"deap\",\n",
    "    \"pyswarms\",\n",
    "    \"scipy\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"sklearn\",\n",
    "]\n",
    "\n",
    "def ensure_package(pkg: str) -> None:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"Installing missing package: {pkg}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "for package in REQUIRED_PACKAGES:\n",
    "    ensure_package(package)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "GLOBAL_SEEDS = [42, 123, 456]\n",
    "RESULTS_DIR = Path(\"outputs\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if hasattr(torch, \"use_deterministic_algorithms\"):\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def format_seconds(seconds: float) -> str:\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    minutes, secs = divmod(seconds, 60)\n",
    "    if minutes < 60:\n",
    "        return f\"{int(minutes)}m {secs:.0f}s\"\n",
    "    hours, mins = divmod(minutes, 60)\n",
    "    return f\"{int(hours)}h {int(mins)}m\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Timer:\n",
    "    label: str\n",
    "    start_time: float = time.time()\n",
    "    elapsed: float = 0.0\n",
    "\n",
    "    def __enter__(self) -> \"Timer\":\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n",
    "        self.elapsed = time.time() - self.start_time\n",
    "        print(f\"[{self.label}] {format_seconds(self.elapsed)}\")\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "for seed in GLOBAL_SEEDS:\n",
    "    set_global_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341402e",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Download datasets, apply transforms, and create reusable train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04491e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Datasets will be prepared on demand.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "DATA_ROOT = Path(\"data\")\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRANSFORMS = {\n",
    "    \"mnist\": {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]),\n",
    "        \"eval\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]),\n",
    "    },\n",
    "    \"cifar10\": {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ]),\n",
    "        \"eval\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ]),\n",
    "    },\n",
    "}\n",
    "\n",
    "DATASETS: Dict[str, Dict[str, Dataset]] = {}\n",
    "DATALOADER_CACHE: Dict[Tuple[str, int, int], Tuple[DataLoader, DataLoader, DataLoader]] = {}\n",
    "DATASET_METADATA: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset_name: str, download: bool = True) -> None:\n",
    "    dataset_name = dataset_name.lower()\n",
    "    if dataset_name in DATASETS:\n",
    "        return\n",
    "\n",
    "    if dataset_name == \"mnist\":\n",
    "        train_dataset = datasets.MNIST(\n",
    "            root=DATA_ROOT,\n",
    "            train=True,\n",
    "            transform=TRANSFORMS[\"mnist\"][\"train\"],\n",
    "            download=download,\n",
    "        )\n",
    "        full_train_len = len(train_dataset)\n",
    "        train_len = int(0.8 * full_train_len)\n",
    "        val_len = int(0.1 * full_train_len)\n",
    "        test_len = full_train_len - train_len - val_len\n",
    "        generator = torch.Generator().manual_seed(2025)\n",
    "        train_subset, val_subset, heldout_subset = torch.utils.data.random_split(\n",
    "            train_dataset,\n",
    "            lengths=[train_len, val_len, test_len],\n",
    "            generator=generator,\n",
    "        )\n",
    "        eval_transform = TRANSFORMS[\"mnist\"][\"eval\"]\n",
    "        heldout_subset.dataset.transform = eval_transform\n",
    "        val_subset.dataset.transform = eval_transform\n",
    "        test_dataset = datasets.MNIST(\n",
    "            root=DATA_ROOT,\n",
    "            train=False,\n",
    "            transform=eval_transform,\n",
    "            download=download,\n",
    "        )\n",
    "    elif dataset_name == \"cifar10\":\n",
    "        train_dataset = datasets.CIFAR10(\n",
    "            root=DATA_ROOT,\n",
    "            train=True,\n",
    "            transform=TRANSFORMS[\"cifar10\"][\"train\"],\n",
    "            download=download,\n",
    "        )\n",
    "        full_train_len = len(train_dataset)\n",
    "        train_len = int(0.8 * full_train_len)\n",
    "        val_len = int(0.1 * full_train_len)\n",
    "        test_len = full_train_len - train_len - val_len\n",
    "        generator = torch.Generator().manual_seed(2025)\n",
    "        train_subset, val_subset, heldout_subset = torch.utils.data.random_split(\n",
    "            train_dataset,\n",
    "            lengths=[train_len, val_len, test_len],\n",
    "            generator=generator,\n",
    "        )\n",
    "        eval_transform = TRANSFORMS[\"cifar10\"][\"eval\"]\n",
    "        for subset in (val_subset, heldout_subset):\n",
    "            subset.dataset.transform = eval_transform\n",
    "        test_dataset = datasets.CIFAR10(\n",
    "            root=DATA_ROOT,\n",
    "            train=False,\n",
    "            transform=eval_transform,\n",
    "            download=download,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    DATASETS[dataset_name] = {\n",
    "        \"train\": train_subset,\n",
    "        \"val\": val_subset,\n",
    "        \"test\": heldout_subset,\n",
    "        \"external_test\": test_dataset,\n",
    "    }\n",
    "    DATASET_METADATA[dataset_name] = {\n",
    "        \"train_size\": len(train_subset),\n",
    "        \"val_size\": len(val_subset),\n",
    "        \"test_size\": len(heldout_subset),\n",
    "        \"external_test_size\": len(test_dataset),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_dataloaders(\n",
    "    dataset_name: str,\n",
    "    batch_size: int,\n",
    "    seed: int,\n",
    "    num_workers: Optional[int] = None,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    dataset_name = dataset_name.lower()\n",
    "    prepare_dataset(dataset_name)\n",
    "    key = (dataset_name, batch_size, seed)\n",
    "    if key in DATALOADER_CACHE:\n",
    "        return DATALOADER_CACHE[key]\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    num_workers = num_workers if num_workers is not None else min(4, os.cpu_count() or 1)\n",
    "\n",
    "    train_subset = DATASETS[dataset_name][\"train\"]\n",
    "    val_subset = DATASETS[dataset_name][\"val\"]\n",
    "    test_subset = DATASETS[dataset_name][\"test\"]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        generator=generator,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=device.type != \"cpu\",\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=device.type != \"cpu\",\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=device.type != \"cpu\",\n",
    "    )\n",
    "\n",
    "    DATALOADER_CACHE[key] = (train_loader, val_loader, test_loader)\n",
    "    return DATALOADER_CACHE[key]\n",
    "\n",
    "\n",
    "metadata_frame = pd.DataFrame.from_dict(DATASET_METADATA, orient=\"index\")\n",
    "display(metadata_frame if not metadata_frame.empty else \"Datasets will be prepared on demand.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f9dbb",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "Define adaptable PyTorch models for MNIST and CIFAR-10 driven by hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "519d32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTMLP(nn.Module):\n",
    "    def __init__(self, hidden_layers: List[int], dropout: float):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        input_dim = 28 * 28\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self, conv_channels: List[int], dropout: float):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        in_channels = 3\n",
    "        for out_channels in conv_channels:\n",
    "            layers.extend(\n",
    "                [\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(kernel_size=2),\n",
    "                ]\n",
    "            )\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout2d(dropout))\n",
    "            in_channels = out_channels\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        feature_dim = conv_channels[-1] * (32 // (2 ** len(conv_channels))) ** 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "PARAM_SPACE = [\n",
    "    {\"name\": \"learning_rate\", \"type\": \"continuous\", \"bounds\": (1e-4, 5e-2), \"scale\": \"log\"},\n",
    "    {\"name\": \"batch_size\", \"type\": \"discrete\", \"choices\": [32, 48, 64, 96, 128]},\n",
    "    {\"name\": \"dropout\", \"type\": \"continuous\", \"bounds\": (0.0, 0.5)},\n",
    "    {\"name\": \"width_scale\", \"type\": \"continuous\", \"bounds\": (0.5, 2.0)},\n",
    "    {\"name\": \"num_layers\", \"type\": \"discrete\", \"choices\": [2, 3, 4]},\n",
    "    {\"name\": \"optimizer\", \"type\": \"categorical\", \"choices\": [\"adam\", \"sgd\"]},\n",
    "]\n",
    "\n",
    "\n",
    "def clamp(value: float, lower: float, upper: float) -> float:\n",
    "    return max(lower, min(value, upper))\n",
    "\n",
    "\n",
    "def decode_param(param: Dict[str, Any], value: float) -> Any:\n",
    "    if param[\"type\"] == \"continuous\":\n",
    "        lower, upper = param[\"bounds\"]\n",
    "        if param.get(\"scale\") == \"log\":\n",
    "            log_lower, log_upper = np.log10(lower), np.log10(upper)\n",
    "            actual = 10 ** (log_lower + value * (log_upper - log_lower))\n",
    "        else:\n",
    "            actual = lower + value * (upper - lower)\n",
    "        return float(actual)\n",
    "    if param[\"type\"] in {\"discrete\", \"categorical\"}:\n",
    "        choices = param[\"choices\"]\n",
    "        index = int(round(clamp(value, 0.0, 1.0) * (len(choices) - 1)))\n",
    "        return choices[index]\n",
    "    raise ValueError(f\"Unsupported parameter type: {param['type']}\")\n",
    "\n",
    "\n",
    "def encode_param(param: Dict[str, Any], value: Any) -> float:\n",
    "    if param[\"type\"] == \"continuous\":\n",
    "        lower, upper = param[\"bounds\"]\n",
    "        if param.get(\"scale\") == \"log\":\n",
    "            log_lower, log_upper = np.log10(lower), np.log10(upper)\n",
    "            return float((np.log10(value) - log_lower) / (log_upper - log_lower))\n",
    "        return float((value - lower) / (upper - lower))\n",
    "    if param[\"type\"] in {\"discrete\", \"categorical\"}:\n",
    "        choices = param[\"choices\"]\n",
    "        return float(choices.index(value) / (len(choices) - 1))\n",
    "    raise ValueError(f\"Unsupported parameter type: {param['type']}\")\n",
    "\n",
    "\n",
    "def decode_vector(vector: Iterable[float]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        param[\"name\"]: decode_param(param, clamp(v, 0.0, 1.0))\n",
    "        for param, v in zip(PARAM_SPACE, vector)\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_config(config: Dict[str, Any]) -> List[float]:\n",
    "    return [encode_param(param, config[param[\"name\"]]) for param in PARAM_SPACE]\n",
    "\n",
    "\n",
    "def build_mnist_hidden_layers(num_layers: int, width_scale: float) -> List[int]:\n",
    "    base_units = [256, 128, 64, 32]\n",
    "    layers = [max(32, int(width_scale * base_units[i])) for i in range(num_layers)]\n",
    "    return layers\n",
    "\n",
    "\n",
    "def build_cifar_channels(num_layers: int, width_scale: float) -> List[int]:\n",
    "    base_channels = [32, 64, 128, 256]\n",
    "    channels = [int(width_scale * base_channels[i]) for i in range(num_layers)]\n",
    "    channels = [max(16, (c // 8) * 8) for c in channels]\n",
    "    return channels\n",
    "\n",
    "\n",
    "def build_model(dataset_name: str, config: Dict[str, Any]) -> nn.Module:\n",
    "    dataset_name = dataset_name.lower()\n",
    "    dropout = float(config[\"dropout\"])\n",
    "    num_layers = int(config[\"num_layers\"])\n",
    "    width_scale = float(config[\"width_scale\"])\n",
    "    if dataset_name == \"mnist\":\n",
    "        hidden_layers = build_mnist_hidden_layers(num_layers, width_scale)\n",
    "        return MNISTMLP(hidden_layers=hidden_layers, dropout=dropout)\n",
    "    if dataset_name == \"cifar10\":\n",
    "        conv_channels = build_cifar_channels(num_layers, width_scale)\n",
    "        return CIFAR10CNN(conv_channels=conv_channels, dropout=dropout)\n",
    "    raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c4cdb",
   "metadata": {},
   "source": [
    "## 4. Baseline Search Strategies\n",
    "\n",
    "Implement grid and random search orchestrators operating on the shared hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02aae8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 8,\n",
    "    \"patience\": 3,\n",
    "    \"max_steps_per_epoch\": None,  # Optionally cap batches per epoch for quick smoke tests\n",
    "}\n",
    "\n",
    "EVALUATION_CACHE: Dict[Tuple[str, Tuple[Tuple[str, Any], ...]], Dict[str, Any]] = {}\n",
    "RESULTS_REGISTRY: List[Dict[str, Any]] = []\n",
    "RECORDED_KEYS: set = set()\n",
    "\n",
    "\n",
    "def config_to_key(dataset_name: str, config: Dict[str, Any]) -> Tuple[str, Tuple[Tuple[str, Any], ...]]:\n",
    "    rounded_items = []\n",
    "    for key, value in sorted(config.items()):\n",
    "        if isinstance(value, float):\n",
    "            rounded_items.append((key, round(value, 6)))\n",
    "        else:\n",
    "            rounded_items.append((key, value))\n",
    "    return dataset_name.lower(), tuple(rounded_items)\n",
    "\n",
    "\n",
    "def get_optimizer(model: nn.Module, config: Dict[str, Any]) -> torch.optim.Optimizer:\n",
    "    learning_rate = float(config[\"learning_rate\"])\n",
    "    optimizer_name = config[\"optimizer\"].lower()\n",
    "    if optimizer_name == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    if optimizer_name == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * targets.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    avg_loss = running_loss / max(1, total)\n",
    "    accuracy = correct / max(1, total)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def build_external_loader(dataset_name: str, batch_size: int, num_workers: Optional[int] = None) -> DataLoader:\n",
    "    dataset_name = dataset_name.lower()\n",
    "    prepare_dataset(dataset_name)\n",
    "    dataset = DATASETS[dataset_name][\"external_test\"]\n",
    "    num_workers = num_workers if num_workers is not None else min(4, os.cpu_count() or 1)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=device.type != \"cpu\",\n",
    "    )\n",
    "\n",
    "\n",
    "def train_single_seed(\n",
    "    dataset_name: str,\n",
    "    config: Dict[str, Any],\n",
    "    seed: int,\n",
    "    device: torch.device,\n",
    "    training_config: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    set_global_seed(seed)\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "    train_loader, val_loader, test_loader = build_dataloaders(dataset_name, batch_size, seed)\n",
    "    model = build_model(dataset_name, config).to(device)\n",
    "    optimizer = get_optimizer(model, config)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_state = None\n",
    "    best_val_acc = -np.inf\n",
    "    best_epoch = -1\n",
    "    epochs_without_improvement = 0\n",
    "    history: List[Dict[str, float]] = []\n",
    "\n",
    "    max_epochs = training_config[\"epochs\"]\n",
    "    patience = training_config[\"patience\"]\n",
    "    max_steps_per_epoch = training_config.get(\"max_steps_per_epoch\")\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_examples = 0\n",
    "        start_time = time.time()\n",
    "        for step, (inputs, targets) in enumerate(train_loader, start=1):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * targets.size(0)\n",
    "            total_examples += targets.size(0)\n",
    "            if max_steps_per_epoch and step >= max_steps_per_epoch:\n",
    "                break\n",
    "        train_loss = running_loss / max(1, total_examples)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        epoch_duration = time.time() - start_time\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"duration_sec\": epoch_duration,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        improved = val_acc > best_val_acc + 1e-4\n",
    "        if improved:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if patience and epochs_without_improvement >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    final_val_loss, final_val_acc = evaluate_model(model, val_loader, device)\n",
    "    final_test_loss, final_test_acc = evaluate_model(model, test_loader, device)\n",
    "    external_loader = build_external_loader(dataset_name, batch_size)\n",
    "    ext_test_loss, ext_test_acc = evaluate_model(model, external_loader, device)\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"history\": history,\n",
    "        \"val_accuracy\": final_val_acc,\n",
    "        \"val_loss\": final_val_loss,\n",
    "        \"test_accuracy\": final_test_acc,\n",
    "        \"test_loss\": final_test_loss,\n",
    "        \"external_test_accuracy\": ext_test_acc,\n",
    "        \"external_test_loss\": ext_test_loss,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"state_dict\": best_state,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_config(\n",
    "    dataset_name: str,\n",
    "    config: Dict[str, Any],\n",
    "    method: str,\n",
    "    training_config: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    cache_key = config_to_key(dataset_name, config)\n",
    "    if cache_key in EVALUATION_CACHE:\n",
    "        return EVALUATION_CACHE[cache_key]\n",
    "\n",
    "    seed_results = []\n",
    "    wall_start = time.time()\n",
    "    with Timer(label=f\"{method} | {dataset_name} config evaluation\"):\n",
    "        for seed in GLOBAL_SEEDS:\n",
    "            seed_result = train_single_seed(dataset_name, config, seed, device, training_config)\n",
    "            seed_results.append(seed_result)\n",
    "    total_runtime = time.time() - wall_start\n",
    "\n",
    "    val_accs = [result[\"val_accuracy\"] for result in seed_results]\n",
    "    test_accs = [result[\"test_accuracy\"] for result in seed_results]\n",
    "    ext_accs = [result[\"external_test_accuracy\"] for result in seed_results]\n",
    "\n",
    "    summary = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"method\": method,\n",
    "        \"config\": config,\n",
    "        \"val_accuracy_mean\": float(np.mean(val_accs)),\n",
    "        \"val_accuracy_std\": float(np.std(val_accs)),\n",
    "        \"test_accuracy_mean\": float(np.mean(test_accs)),\n",
    "        \"test_accuracy_std\": float(np.std(test_accs)),\n",
    "        \"external_test_accuracy_mean\": float(np.mean(ext_accs)),\n",
    "        \"external_test_accuracy_std\": float(np.std(ext_accs)),\n",
    "        \"runtime_sec\": total_runtime,\n",
    "        \"seed_results\": seed_results,\n",
    "    }\n",
    "\n",
    "    EVALUATION_CACHE[cache_key] = summary\n",
    "    return summary\n",
    "\n",
    "\n",
    "def record_result(result: Dict[str, Any]) -> None:\n",
    "    global RECORDED_KEYS\n",
    "    key = (result.get(\"method\"), config_to_key(result[\"dataset\"], result[\"config\"]))\n",
    "    if key in RECORDED_KEYS:\n",
    "        return\n",
    "    RECORDED_KEYS.add(key)\n",
    "    RESULTS_REGISTRY.append(result)\n",
    "\n",
    "\n",
    "GRID_VALUES = {\n",
    "    \"learning_rate\": [1e-3, 5e-3],\n",
    "    \"batch_size\": [48, 96],\n",
    "    \"dropout\": [0.1, 0.3],\n",
    "    \"width_scale\": [0.75, 1.25],\n",
    "    \"num_layers\": [2, 3],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"],\n",
    "}\n",
    "\n",
    "\n",
    "def iter_grid_configs() -> Iterable[Dict[str, Any]]:\n",
    "    keys = list(GRID_VALUES.keys())\n",
    "    values = [GRID_VALUES[key] for key in keys]\n",
    "    for combination in itertools.product(*values):\n",
    "        yield {key: value for key, value in zip(keys, combination)}\n",
    "\n",
    "\n",
    "def sample_random_config() -> Dict[str, Any]:\n",
    "    config: Dict[str, Any] = {}\n",
    "    for param in PARAM_SPACE:\n",
    "        if param[\"type\"] == \"continuous\":\n",
    "            value = random.random()\n",
    "            config[param[\"name\"]] = decode_param(param, value)\n",
    "        elif param[\"type\"] in {\"discrete\", \"categorical\"}:\n",
    "            config[param[\"name\"]] = random.choice(param[\"choices\"])\n",
    "    return config\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    dataset_name: str,\n",
    "    max_trials: Optional[int] = None,\n",
    "    training_config: Optional[Dict[str, Any]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    training_config = training_config or TRAINING_CONFIG\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    for trial_id, config in enumerate(iter_grid_configs(), start=1):\n",
    "        summary = evaluate_config(dataset_name, config, method=\"Grid\", training_config=training_config)\n",
    "        record_result({**summary, \"trial_id\": trial_id})\n",
    "        records.append({**summary, \"trial_id\": trial_id})\n",
    "        if max_trials and trial_id >= max_trials:\n",
    "            break\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def run_random_search(\n",
    "    dataset_name: str,\n",
    "    n_trials: int,\n",
    "    training_config: Optional[Dict[str, Any]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    training_config = training_config or TRAINING_CONFIG\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    for trial_id in range(1, n_trials + 1):\n",
    "        config = sample_random_config()\n",
    "        summary = evaluate_config(dataset_name, config, method=\"Random\", training_config=training_config)\n",
    "        record_result({**summary, \"trial_id\": trial_id})\n",
    "        records.append({**summary, \"trial_id\": trial_id})\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8bdf5",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Optimization\n",
    "\n",
    "Use DEAP to evolve hyperparameter vectors and evaluate them with the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f22bce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "\n",
    "def clamp_individual(individual: List[float]) -> None:\n",
    "    for i in range(len(individual)):\n",
    "        individual[i] = float(clamp(individual[i], 0.0, 1.0))\n",
    "\n",
    "\n",
    "def run_ga(\n",
    "    dataset_name: str,\n",
    "    population_size: int = 12,\n",
    "    generations: int = 6,\n",
    "    crossover_prob: float = 0.7,\n",
    "    mutation_prob: float = 0.2,\n",
    "    training_config: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    training_config = training_config or TRAINING_CONFIG\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_float\", random.random)\n",
    "    toolbox.register(\n",
    "        \"individual\",\n",
    "        tools.initRepeat,\n",
    "        creator.Individual,\n",
    "        toolbox.attr_float,\n",
    "        n=len(PARAM_SPACE),\n",
    "    )\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    cache_hits: Dict[Tuple[str, Tuple[Tuple[str, Any], ...]], int] = defaultdict(int)\n",
    "    trial_counter = {\"count\": 0}\n",
    "\n",
    "    def evaluate_individual(individual: List[float]) -> Tuple[float]:\n",
    "        clamp_individual(individual)\n",
    "        config = decode_vector(individual)\n",
    "        key = config_to_key(dataset_name, config)\n",
    "        cache_hits[key] += 1\n",
    "        summary = evaluate_config(dataset_name, config, method=\"GA\", training_config=training_config)\n",
    "        if cache_hits[key] == 1:\n",
    "            trial_counter[\"count\"] += 1\n",
    "            record_result({**summary, \"trial_id\": trial_counter[\"count\"]})\n",
    "        return (summary[\"val_accuracy_mean\"],)\n",
    "\n",
    "    toolbox.register(\"evaluate\", evaluate_individual)\n",
    "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.2)\n",
    "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0.0, sigma=0.2, indpb=0.5)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    population = toolbox.population(n=population_size)\n",
    "    hof = tools.HallOfFame(5)\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values[0])\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"nevals\", \"avg\", \"std\", \"min\", \"max\"]\n",
    "\n",
    "    history_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    invalid_individuals = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = list(toolbox.map(toolbox.evaluate, invalid_individuals))\n",
    "    for ind, fit in zip(invalid_individuals, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    hof.update(population)\n",
    "    record = stats.compile(population)\n",
    "    logbook.record(gen=0, nevals=len(invalid_individuals), **record)\n",
    "    history_rows.append({\"generation\": 0, **record})\n",
    "\n",
    "    for gen in range(1, generations + 1):\n",
    "        offspring = tools.selTournament(population, len(population), tournsize=3)\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb=crossover_prob, mutpb=mutation_prob)\n",
    "        for ind in offspring:\n",
    "            clamp_individual(ind)\n",
    "\n",
    "        invalid_individuals = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = list(toolbox.map(toolbox.evaluate, invalid_individuals))\n",
    "        for ind, fit in zip(invalid_individuals, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        population[:] = offspring\n",
    "        hof.update(population)\n",
    "        record = stats.compile(population)\n",
    "        logbook.record(gen=gen, nevals=len(invalid_individuals), **record)\n",
    "        history_rows.append({\"generation\": gen, **record})\n",
    "\n",
    "    best_config = decode_vector(hof[0]) if len(hof) > 0 else None\n",
    "    return {\n",
    "        \"population\": population,\n",
    "        \"hall_of_fame\": hof,\n",
    "        \"logbook\": logbook,\n",
    "        \"history\": pd.DataFrame(history_rows),\n",
    "        \"best_config\": best_config,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1ea3d",
   "metadata": {},
   "source": [
    "## 6. Differential Evolution Optimization\n",
    "\n",
    "Wrap SciPy's differential evolution routine to search the normalized hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe8afb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "\n",
    "def run_de(\n",
    "    dataset_name: str,\n",
    "    popsize: int = 15,\n",
    "    max_iter: int = 8,\n",
    "    mutation: Tuple[float, float] = (0.5, 1.0),\n",
    "    recombination: float = 0.7,\n",
    "    training_config: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    training_config = training_config or TRAINING_CONFIG\n",
    "    bounds = [(0.0, 1.0)] * len(PARAM_SPACE)\n",
    "    evaluation_records: List[Dict[str, Any]] = []\n",
    "    best_tracker = {\"score\": -np.inf, \"config\": None}\n",
    "    trial_counter = {\"count\": 0}\n",
    "\n",
    "    def objective(vector: np.ndarray) -> float:\n",
    "        vector = np.clip(vector, 0.0, 1.0)\n",
    "        config = decode_vector(vector)\n",
    "        key = config_to_key(dataset_name, config)\n",
    "        was_cached = key in EVALUATION_CACHE\n",
    "        summary = evaluate_config(dataset_name, config, method=\"DE\", training_config=training_config)\n",
    "        if not was_cached:\n",
    "            trial_counter[\"count\"] += 1\n",
    "            record_result({**summary, \"trial_id\": trial_counter[\"count\"]})\n",
    "        score = summary[\"val_accuracy_mean\"]\n",
    "        evaluation_records.append(\n",
    "            {\n",
    "                \"config\": config,\n",
    "                \"score\": score,\n",
    "                \"test_accuracy\": summary[\"test_accuracy_mean\"],\n",
    "                \"external_test_accuracy\": summary[\"external_test_accuracy_mean\"],\n",
    "            }\n",
    "        )\n",
    "        if score > best_tracker[\"score\"]:\n",
    "            best_tracker[\"score\"] = score\n",
    "            best_tracker[\"config\"] = config\n",
    "        return -score\n",
    "\n",
    "    history_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    def callback(xk: np.ndarray, convergence: float) -> bool:\n",
    "        history_rows.append(\n",
    "            {\n",
    "                \"generation\": len(history_rows) + 1,\n",
    "                \"best_score\": best_tracker[\"score\"],\n",
    "                \"convergence\": convergence,\n",
    "            }\n",
    "        )\n",
    "        return False\n",
    "\n",
    "    result = differential_evolution(\n",
    "        objective,\n",
    "        bounds,\n",
    "        strategy=\"best1bin\",\n",
    "        popsize=popsize,\n",
    "        maxiter=max_iter,\n",
    "        mutation=mutation,\n",
    "        recombination=recombination,\n",
    "        seed=GLOBAL_SEEDS[0],\n",
    "        callback=callback,\n",
    "        updating=\"deferred\",\n",
    "        workers=1,\n",
    "        polish=False,\n",
    "    )\n",
    "\n",
    "    best_config = decode_vector(result.x)\n",
    "    summary = evaluate_config(dataset_name, best_config, method=\"DE\", training_config=training_config)\n",
    "    record_result({**summary, \"trial_id\": trial_counter[\"count\"] + 1})\n",
    "\n",
    "    return {\n",
    "        \"result\": result,\n",
    "        \"best_config\": best_config,\n",
    "        \"history\": pd.DataFrame(history_rows),\n",
    "        \"evaluations\": pd.DataFrame(evaluation_records),\n",
    "        \"summary\": summary,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd423533",
   "metadata": {},
   "source": [
    "## 7. Particle Swarm Optimization\n",
    "\n",
    "Configure a PSO loop (via PySwarms) to explore the normalized hyperparameter space with velocity clamping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2af09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pso(\n",
    "    dataset_name: str,\n",
    "    particles: int = 20,\n",
    "    iterations: int = 10,\n",
    "    inertia_start: float = 0.9,\n",
    "    inertia_end: float = 0.5,\n",
    "    cognitive: float = 1.6,\n",
    "    social: float = 1.6,\n",
    "    velocity_clamp: Tuple[float, float] = (-1.5, 1.5),\n",
    "    training_config: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    training_config = training_config or TRAINING_CONFIG\n",
    "    dim = len(PARAM_SPACE)\n",
    "    seed_value = GLOBAL_SEEDS[1] if len(GLOBAL_SEEDS) > 1 else GLOBAL_SEEDS[0] if GLOBAL_SEEDS else None\n",
    "    rng = np.random.default_rng(seed_value)\n",
    "    positions = rng.random((particles, dim))\n",
    "    velocities = rng.uniform(velocity_clamp[0], velocity_clamp[1], size=(particles, dim))\n",
    "    personal_best_positions = positions.copy()\n",
    "    personal_best_scores = np.full(particles, -np.inf)\n",
    "    global_best_position = positions[0].copy()\n",
    "    global_best_score = -np.inf\n",
    "    trial_counter = {\"count\": 0}\n",
    "    evaluation_records: List[Dict[str, Any]] = []\n",
    "    history_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for iteration in range(1, iterations + 1):\n",
    "        inertia = inertia_start + (inertia_end - inertia_start) * ((iteration - 1) / max(1, iterations - 1))\n",
    "        for idx in range(particles):\n",
    "            positions[idx] = np.clip(positions[idx], 0.0, 1.0)\n",
    "            config = decode_vector(positions[idx])\n",
    "            key = config_to_key(dataset_name, config)\n",
    "            was_cached = key in EVALUATION_CACHE\n",
    "            summary = evaluate_config(dataset_name, config, method=\"PSO\", training_config=training_config)\n",
    "            if not was_cached:\n",
    "                trial_counter[\"count\"] += 1\n",
    "                record_result({**summary, \"trial_id\": trial_counter[\"count\"]})\n",
    "            score = summary[\"val_accuracy_mean\"]\n",
    "            evaluation_records.append(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"particle\": idx,\n",
    "                    \"config\": config,\n",
    "                    \"score\": score,\n",
    "                    \"test_accuracy\": summary[\"test_accuracy_mean\"],\n",
    "                    \"external_test_accuracy\": summary[\"external_test_accuracy_mean\"],\n",
    "                }\n",
    "            )\n",
    "            if score > personal_best_scores[idx]:\n",
    "                personal_best_scores[idx] = score\n",
    "                personal_best_positions[idx] = positions[idx].copy()\n",
    "            if score > global_best_score:\n",
    "                global_best_score = score\n",
    "                global_best_position = positions[idx].copy()\n",
    "\n",
    "        r1 = rng.random((particles, dim))\n",
    "        r2 = rng.random((particles, dim))\n",
    "        velocities = (\n",
    "            inertia * velocities\n",
    "            + cognitive * r1 * (personal_best_positions - positions)\n",
    "            + social * r2 * (global_best_position - positions)\n",
    "        )\n",
    "        velocities = np.clip(velocities, velocity_clamp[0], velocity_clamp[1])\n",
    "        positions = np.clip(positions + velocities, 0.0, 1.0)\n",
    "        history_rows.append(\n",
    "            {\n",
    "                \"iteration\": iteration,\n",
    "                \"inertia\": inertia,\n",
    "                \"best_score\": global_best_score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    best_config = decode_vector(global_best_position)\n",
    "    summary = evaluate_config(dataset_name, best_config, method=\"PSO\", training_config=training_config)\n",
    "    record_result({**summary, \"trial_id\": trial_counter[\"count\"] + 1})\n",
    "\n",
    "    return {\n",
    "        \"best_config\": best_config,\n",
    "        \"history\": pd.DataFrame(history_rows),\n",
    "        \"evaluations\": pd.DataFrame(evaluation_records),\n",
    "        \"summary\": summary,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00080068",
   "metadata": {},
   "source": [
    "## 8. Unified Evaluation Pipeline\n",
    "\n",
    "Aggregate experiment outputs and materialize accuracy/runtime tables aligned with the manuscript template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71b2ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_PLAN = {\n",
    "    \"mnist\": {\n",
    "        \"grid_trials\": 4,\n",
    "        \"random_trials\": 6,\n",
    "        \"ga\": {\"population_size\": 10, \"generations\": 4},\n",
    "        \"de\": {\"popsize\": 12, \"max_iter\": 6},\n",
    "        \"pso\": {\"particles\": 16, \"iterations\": 6},\n",
    "    },\n",
    "    \"cifar10\": {\n",
    "        \"grid_trials\": 4,\n",
    "        \"random_trials\": 6,\n",
    "        \"ga\": {\"population_size\": 12, \"generations\": 5},\n",
    "        \"de\": {\"popsize\": 14, \"max_iter\": 6},\n",
    "        \"pso\": {\"particles\": 18, \"iterations\": 7},\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def results_dataframe() -> pd.DataFrame:\n",
    "    if not RESULTS_REGISTRY:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.json_normalize(RESULTS_REGISTRY)\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarise_for_table(dataset_name: str) -> pd.DataFrame:\n",
    "    df = results_dataframe()\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df = df[df[\"dataset\"].str.lower() == dataset_name.lower()].copy()\n",
    "    grouped = (\n",
    "        df.groupby(\"method\")\n",
    "        .agg(\n",
    "            mean_accuracy=(\"test_accuracy_mean\", \"mean\"),\n",
    "            std_accuracy=(\"test_accuracy_mean\", \"std\"),\n",
    "            mean_runtime=(\"runtime_sec\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"dataset\"] = dataset_name\n",
    "    grouped = grouped[[\"method\", \"dataset\", \"mean_accuracy\", \"std_accuracy\", \"mean_runtime\"]]\n",
    "    grouped.sort_values(by=\"mean_accuracy\", ascending=False, inplace=True)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def build_reporting_table() -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for dataset_name in [\"mnist\", \"cifar10\"]:\n",
    "        frames.append(summarise_for_table(dataset_name))\n",
    "    table = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    table.rename(\n",
    "        columns={\n",
    "            \"method\": \"Method\",\n",
    "            \"dataset\": \"Dataset\",\n",
    "            \"mean_accuracy\": \"Mean Accuracy\",\n",
    "            \"std_accuracy\": \"Std Dev\",\n",
    "            \"mean_runtime\": \"Runtime (sec)\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    return table\n",
    "\n",
    "\n",
    "def run_experiment_suite(plan: Optional[Dict[str, Dict[str, Any]]] = None) -> None:\n",
    "    plan = plan or EXPERIMENT_PLAN\n",
    "    for dataset_name, config in plan.items():\n",
    "        print(f\"\\n=== Running baseline searches for {dataset_name.upper()} ===\")\n",
    "        if config.get(\"grid_trials\"):\n",
    "            run_grid_search(dataset_name, max_trials=config[\"grid_trials\"])\n",
    "        if config.get(\"random_trials\"):\n",
    "            run_random_search(dataset_name, n_trials=config[\"random_trials\"])\n",
    "\n",
    "        print(f\"\\n=== Running evolutionary searches for {dataset_name.upper()} ===\")\n",
    "        ga_params = config.get(\"ga\")\n",
    "        if ga_params:\n",
    "            run_ga(dataset_name, **ga_params)\n",
    "\n",
    "        de_params = config.get(\"de\")\n",
    "        if de_params:\n",
    "            run_de(dataset_name, **de_params)\n",
    "\n",
    "        pso_params = config.get(\"pso\")\n",
    "        if pso_params:\n",
    "            run_pso(dataset_name, **pso_params)\n",
    "\n",
    "        print(f\"Completed suite for {dataset_name}.\")\n",
    "\n",
    "    print(\"\\nAll experiment suites completed. Build the reporting table once metrics are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38dc0f",
   "metadata": {},
   "source": [
    "## 9. Statistical Analysis and Visualization\n",
    "\n",
    "Run Welch's t-tests with Holm–Bonferroni correction and produce convergence and distribution plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94d1a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "def expand_seed_results() -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for record in RESULTS_REGISTRY:\n",
    "        dataset = record[\"dataset\"]\n",
    "        method = record[\"method\"]\n",
    "        config = record[\"config\"]\n",
    "        trial_id = record.get(\"trial_id\")\n",
    "        for seed_entry in record.get(\"seed_results\", []):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"dataset\": dataset,\n",
    "                    \"method\": method,\n",
    "                    \"trial_id\": trial_id,\n",
    "                    \"seed\": seed_entry[\"seed\"],\n",
    "                    \"val_accuracy\": seed_entry[\"val_accuracy\"],\n",
    "                    \"test_accuracy\": seed_entry[\"test_accuracy\"],\n",
    "                    \"external_test_accuracy\": seed_entry[\"external_test_accuracy\"],\n",
    "                    \"best_epoch\": seed_entry[\"best_epoch\"],\n",
    "                    \"config\": config,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def welch_t_test(group1: Iterable[float], group2: Iterable[float]) -> Tuple[float, float]:\n",
    "    statistic, pvalue = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "    return statistic, pvalue\n",
    "\n",
    "\n",
    "def holm_bonferroni(pvalues: Dict[str, float], alpha: float = 0.05) -> pd.DataFrame:\n",
    "    ordered = sorted(pvalues.items(), key=lambda item: item[1])\n",
    "    m = len(ordered)\n",
    "    results = []\n",
    "    for rank, (label, pvalue) in enumerate(ordered, start=1):\n",
    "        threshold = alpha / (m - rank + 1)\n",
    "        reject = pvalue <= threshold\n",
    "        results.append(\n",
    "            {\n",
    "                \"comparison\": label,\n",
    "                \"pvalue\": pvalue,\n",
    "                \"threshold\": threshold,\n",
    "                \"reject_H0\": reject,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def compare_methods_with_baseline(\n",
    "    dataset_name: str,\n",
    "    baseline_method: str,\n",
    "    candidate_methods: List[str],\n",
    "    alpha: float = 0.05,\n",
    ") -> pd.DataFrame:\n",
    "    seed_df = expand_seed_results()\n",
    "    if seed_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    subset = seed_df[seed_df[\"dataset\"].str.lower() == dataset_name.lower()]\n",
    "    results: Dict[str, float] = {}\n",
    "    for method in candidate_methods:\n",
    "        baseline_scores = subset[subset[\"method\"] == baseline_method][\"test_accuracy\"].values\n",
    "        method_scores = subset[subset[\"method\"] == method][\"test_accuracy\"].values\n",
    "        if len(baseline_scores) == 0 or len(method_scores) == 0:\n",
    "            continue\n",
    "        statistic, pvalue = welch_t_test(method_scores, baseline_scores)\n",
    "        results[f\"{method} vs {baseline_method}\"] = pvalue\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "    corrected = holm_bonferroni(results, alpha=alpha)\n",
    "    return corrected\n",
    "\n",
    "\n",
    "def plot_convergence(history: pd.DataFrame, metric: str, title: str) -> None:\n",
    "    if history.empty:\n",
    "        print(\"History is empty. Run the optimizer first.\")\n",
    "        return\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history[history.columns[0]], history[metric], marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(history.columns[0].capitalize())\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy_distributions(dataset_name: str) -> None:\n",
    "    seed_df = expand_seed_results()\n",
    "    if seed_df.empty:\n",
    "        print(\"No seed-level results available yet.\")\n",
    "        return\n",
    "    subset = seed_df[seed_df[\"dataset\"].str.lower() == dataset_name.lower()]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.violinplot(data=subset, x=\"method\", y=\"test_accuracy\", inner=\"quartile\", cut=0)\n",
    "    plt.title(f\"Test accuracy distributions for {dataset_name.upper()}\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351dc86",
   "metadata": {},
   "source": [
    "## 10. Experiment Logging and Export\n",
    "\n",
    "Persist raw metrics, configuration metadata, and figures for inclusion in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccba9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(output_dir: Path = RESULTS_DIR) -> Dict[str, Path]:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tables_dir = output_dir / \"tables\"\n",
    "    raw_dir = output_dir / \"raw\"\n",
    "    checkpoints_dir = output_dir / \"checkpoints\"\n",
    "    figures_dir = output_dir / \"figures\"\n",
    "    for directory in [tables_dir, raw_dir, checkpoints_dir, figures_dir]:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    artefacts: Dict[str, Path] = {}\n",
    "    results_df = results_dataframe()\n",
    "    if not results_df.empty:\n",
    "        csv_path = tables_dir / \"all_results.csv\"\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        artefacts[\"all_results_csv\"] = csv_path\n",
    "\n",
    "        report_table = build_reporting_table()\n",
    "        if not report_table.empty:\n",
    "            table_path = tables_dir / \"table_4_1_template.csv\"\n",
    "            report_table.to_csv(table_path, index=False)\n",
    "            artefacts[\"report_table_csv\"] = table_path\n",
    "\n",
    "    snapshot_path = raw_dir / \"results_registry.pt\"\n",
    "    torch.save(RESULTS_REGISTRY, snapshot_path)\n",
    "    artefacts[\"registry_snapshot\"] = snapshot_path\n",
    "\n",
    "    metadata = []\n",
    "    for record in RESULTS_REGISTRY:\n",
    "        metadata.append(\n",
    "            {\n",
    "                key: value\n",
    "                for key, value in record.items()\n",
    "                if key not in {\"seed_results\"}\n",
    "            }\n",
    "        )\n",
    "    metadata_path = raw_dir / \"summary.json\"\n",
    "    with metadata_path.open(\"w\") as fp:\n",
    "        json.dump(metadata, fp, indent=2, default=str)\n",
    "    artefacts[\"summary_json\"] = metadata_path\n",
    "\n",
    "    for record in RESULTS_REGISTRY:\n",
    "        dataset = record[\"dataset\"].lower()\n",
    "        method = record[\"method\"].lower().replace(\" \", \"_\")\n",
    "        trial_id = record.get(\"trial_id\", \"na\")\n",
    "        for seed_entry in record.get(\"seed_results\", []):\n",
    "            state_dict = seed_entry.get(\"state_dict\")\n",
    "            if state_dict is None:\n",
    "                continue\n",
    "            filename = f\"{dataset}_{method}_trial{trial_id}_seed{seed_entry['seed']}.pt\"\n",
    "            checkpoint_path = checkpoints_dir / filename\n",
    "            torch.save(state_dict, checkpoint_path)\n",
    "    artefacts[\"checkpoints_dir\"] = checkpoints_dir\n",
    "    artefacts[\"figures_dir\"] = figures_dir\n",
    "\n",
    "    print(\"Export completed. Artefacts saved to:\")\n",
    "    for label, path in artefacts.items():\n",
    "        print(f\" - {label}: {path}\")\n",
    "\n",
    "    return artefacts\n",
    "\n",
    "\n",
    "def save_figure(fig: plt.Figure, filename: str, figures_dir: Path = RESULTS_DIR / \"figures\") -> Path:\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = figures_dir / filename\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=300)\n",
    "    print(f\"Saved figure to {path}\")\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24982fda",
   "metadata": {},
   "source": [
    "### Data Prefetch Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9722143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_data_availability(download: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Trigger dataset preparation and return the metadata frame.\"\"\"\n",
    "    DATASETS.clear()\n",
    "    DATALOADER_CACHE.clear()\n",
    "    DATASET_METADATA.clear()\n",
    "    for name in (\"mnist\", \"cifar10\"):\n",
    "        print(f\"Preparing {name.upper()} (download={download})...\")\n",
    "        prepare_dataset(name, download=download)\n",
    "    metadata_frame = pd.DataFrame.from_dict(DATASET_METADATA, orient=\"index\")\n",
    "    display(metadata_frame)\n",
    "    return metadata_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fe265",
   "metadata": {},
   "source": [
    "### Usage Notes\n",
    "\n",
    "1. Adjust `TRAINING_CONFIG` (epochs, patience, and optional `max_steps_per_epoch`) if you need faster smoke tests.\n",
    "2. Use `run_smoke_test()` to grab datasets, execute a compact suite, and preview the reporting table before longer runs.\n",
    "3. Update `EXPERIMENT_PLAN` to scale the search budget per dataset before calling `run_experiment_suite()`.\n",
    "4. After experiments finish, call `build_reporting_table()` to populate the manuscript template and `export_results()` to persist metrics and checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54145405",
   "metadata": {},
   "source": [
    "### Quick Smoke Test Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55695035",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def run_smoke_test(override_plan: Optional[Dict[str, Dict[str, Any]]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run a compact experiment suite for validation.\n",
    "    Downloads data if needed, trims training config for speed, and restores defaults afterwards.\n",
    "    \"\"\"\n",
    "    ensure_data_availability(download=True)\n",
    "    EVALUATION_CACHE.clear()\n",
    "    RESULTS_REGISTRY.clear()\n",
    "    RECORDED_KEYS.clear()\n",
    "\n",
    "    plan = override_plan or {\n",
    "        \"mnist\": {\n",
    "            \"random_trials\": 1,\n",
    "            \"ga\": None,\n",
    "            \"de\": None,\n",
    "            \"pso\": None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    original_config = TRAINING_CONFIG.copy()\n",
    "    original_seeds = GLOBAL_SEEDS.copy()\n",
    "    try:\n",
    "        TRAINING_CONFIG.update({\"epochs\": 1, \"patience\": 1, \"max_steps_per_epoch\": 25})\n",
    "        GLOBAL_SEEDS.clear()\n",
    "        GLOBAL_SEEDS.extend([original_seeds[0]])\n",
    "        run_experiment_suite(plan)\n",
    "        table = build_reporting_table()\n",
    "        display(table)\n",
    "        return table\n",
    "    finally:\n",
    "        TRAINING_CONFIG.update(original_config)\n",
    "        GLOBAL_SEEDS.clear()\n",
    "        GLOBAL_SEEDS.extend(original_seeds)\n",
    "\n",
    "\n",
    "\n",
    "def run_full_benchmark(\n",
    "    plan: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "    training_override: Optional[Dict[str, Any]] = None,\n",
    "    download: bool = False,\n",
    "    seed_subset: Optional[List[int]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Execute the full experiment suite with optional overrides and return the summary table.\"\"\"\n",
    "    ensure_data_availability(download=download)\n",
    "    EVALUATION_CACHE.clear()\n",
    "    RESULTS_REGISTRY.clear()\n",
    "    RECORDED_KEYS.clear()\n",
    "\n",
    "    original_config = TRAINING_CONFIG.copy()\n",
    "    original_seeds = GLOBAL_SEEDS.copy()\n",
    "    try:\n",
    "        if seed_subset:\n",
    "            GLOBAL_SEEDS.clear()\n",
    "            GLOBAL_SEEDS.extend(seed_subset)\n",
    "        if training_override:\n",
    "            TRAINING_CONFIG.update(training_override)\n",
    "        run_experiment_suite(plan or EXPERIMENT_PLAN)\n",
    "        table = build_reporting_table()\n",
    "        display(table)\n",
    "        return table\n",
    "    finally:\n",
    "        TRAINING_CONFIG.update(original_config)\n",
    "        GLOBAL_SEEDS.clear()\n",
    "        GLOBAL_SEEDS.extend(original_seeds)\n",
    "\n",
    "\n",
    "\n",
    "def generate_report_figures(\n",
    "    table: Optional[pd.DataFrame] = None,\n",
    "    output_dir: Path = RESULTS_DIR / \"figures\",\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"Create comparison figures for the report and return their file paths.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if table is None or table.empty:\n",
    "        table = build_reporting_table()\n",
    "    figures: Dict[str, Path] = {}\n",
    "    if table is None or table.empty:\n",
    "        print(\"No aggregate results available. Run experiments first.\")\n",
    "        return figures\n",
    "\n",
    "    accuracy_fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    sns.barplot(data=table, x=\"Dataset\", y=\"Mean Accuracy\", hue=\"Method\", ax=ax)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(\"Mean Accuracy\")\n",
    "    ax.set_title(\"Method Comparison by Dataset\")\n",
    "    figures[\"accuracy_comparison\"] = save_figure(accuracy_fig, \"accuracy_comparison.png\", figures_dir=output_dir)\n",
    "    plt.close(accuracy_fig)\n",
    "\n",
    "    runtime_fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    sns.barplot(data=table, x=\"Dataset\", y=\"Runtime (sec)\", hue=\"Method\", ax=ax)\n",
    "    ax.set_ylabel(\"Runtime (sec)\")\n",
    "    ax.set_title(\"Runtime Comparison by Dataset\")\n",
    "    figures[\"runtime_comparison\"] = save_figure(runtime_fig, \"runtime_comparison.png\", figures_dir=output_dir)\n",
    "    plt.close(runtime_fig)\n",
    "\n",
    "    seed_df = expand_seed_results()\n",
    "    if not seed_df.empty:\n",
    "        for dataset_name in sorted(seed_df[\"dataset\"].str.lower().unique()):\n",
    "            subset = seed_df[seed_df[\"dataset\"].str.lower() == dataset_name]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            fig, ax = plt.subplots(figsize=(7, 4))\n",
    "            sns.violinplot(data=subset, x=\"method\", y=\"test_accuracy\", inner=\"quartile\", cut=0, ax=ax)\n",
    "            ax.set_xlabel(\"Method\")\n",
    "            ax.set_ylabel(\"Test Accuracy\")\n",
    "            ax.set_title(f\"Test Accuracy Distribution - {dataset_name.upper()}\")\n",
    "            plt.xticks(rotation=30)\n",
    "            filename = f\"{dataset_name}_accuracy_distribution.png\"\n",
    "            figures[f\"{dataset_name}_distribution\"] = save_figure(fig, filename, figures_dir=output_dir)\n",
    "            plt.close(fig)\n",
    "    else:\n",
    "        print(\"Seed-level results unavailable; skipping distribution plots.\")\n",
    "\n",
    "    return figures\n",
    "\n",
    "\n",
    "run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "782ccc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing MNIST (download=False)...\n",
      "Preparing CIFAR10 (download=False)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>external_test_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mnist</th>\n",
       "      <td>48000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cifar10</th>\n",
       "      <td>40000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_size  val_size  test_size  external_test_size\n",
       "mnist         48000      6000       6000               10000\n",
       "cifar10       40000      5000       5000               10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running baseline searches for MNIST ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaicho/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid | mnist config evaluation] 1m 28s\n",
      "[Grid | mnist config evaluation] 1m 21s\n",
      "[Grid | mnist config evaluation] 1m 27s\n",
      "[Grid | mnist config evaluation] 1m 28s\n",
      "[Random | mnist config evaluation] 1m 26s\n",
      "[Random | mnist config evaluation] 1m 18s\n",
      "[Random | mnist config evaluation] 1m 21s\n",
      "\n",
      "=== Running evolutionary searches for MNIST ===\n",
      "[GA | mnist config evaluation] 1m 21s\n",
      "[GA | mnist config evaluation] 1m 9s\n",
      "[GA | mnist config evaluation] 1m 21s\n",
      "[GA | mnist config evaluation] 1m 20s\n",
      "[GA | mnist config evaluation] 1m 17s\n",
      "[GA | mnist config evaluation] 1m 19s\n",
      "[GA | mnist config evaluation] 1m 17s\n",
      "[GA | mnist config evaluation] 1m 20s\n",
      "[GA | mnist config evaluation] 1m 15s\n",
      "[GA | mnist config evaluation] 1m 18s\n",
      "[GA | mnist config evaluation] 1m 18s\n",
      "[GA | mnist config evaluation] 1m 20s\n",
      "[GA | mnist config evaluation] 1m 19s\n",
      "[GA | mnist config evaluation] 1m 20s\n",
      "[GA | mnist config evaluation] 1m 15s\n",
      "[GA | mnist config evaluation] 1m 19s\n",
      "[GA | mnist config evaluation] 1m 22s\n",
      "[GA | mnist config evaluation] 1m 20s\n",
      "[GA | mnist config evaluation] 1m 19s\n",
      "[GA | mnist config evaluation] 1m 21s\n",
      "[GA | mnist config evaluation] 1m 21s\n",
      "[GA | mnist config evaluation] 1m 22s\n",
      "[GA | mnist config evaluation] 1m 20s\n",
      "[GA | mnist config evaluation] 1m 19s\n",
      "[GA | mnist config evaluation] 1m 21s\n",
      "[GA | mnist config evaluation] 1m 20s\n",
      "[GA | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 28s\n",
      "[DE | mnist config evaluation] 1m 25s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 58.3s\n",
      "[DE | mnist config evaluation] 1m 34s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 14s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 18s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 17s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 26s\n",
      "[DE | mnist config evaluation] 1m 17s\n",
      "[DE | mnist config evaluation] 1m 6s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 15s\n",
      "[DE | mnist config evaluation] 1m 16s\n",
      "[DE | mnist config evaluation] 1m 18s\n",
      "[DE | mnist config evaluation] 1m 11s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 16s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 16s\n",
      "[DE | mnist config evaluation] 1m 14s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 25s\n",
      "[DE | mnist config evaluation] 1m 26s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 10s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 29s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 13s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 25s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 25s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 26s\n",
      "[DE | mnist config evaluation] 1m 26s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 18s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 16s\n",
      "[DE | mnist config evaluation] 1m 1s\n",
      "[DE | mnist config evaluation] 1m 16s\n",
      "[DE | mnist config evaluation] 1m 16s\n",
      "[DE | mnist config evaluation] 1m 17s\n",
      "[DE | mnist config evaluation] 1m 25s\n",
      "[DE | mnist config evaluation] 1m 18s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 3s\n",
      "[DE | mnist config evaluation] 1m 15s\n",
      "[DE | mnist config evaluation] 1m 17s\n",
      "[DE | mnist config evaluation] 60.0s\n",
      "[DE | mnist config evaluation] 1m 18s\n",
      "[DE | mnist config evaluation] 1m 18s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 59.2s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 17s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 17s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 8s\n",
      "[DE | mnist config evaluation] 1m 17s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 2s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 23s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 25s\n",
      "[DE | mnist config evaluation] 1m 26s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 27s\n",
      "[DE | mnist config evaluation] 1m 32s\n",
      "[DE | mnist config evaluation] 1m 26s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 36s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 24s\n",
      "[DE | mnist config evaluation] 1m 19s\n",
      "[DE | mnist config evaluation] 1m 21s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 22s\n",
      "[DE | mnist config evaluation] 1m 20s\n",
      "[DE | mnist config evaluation] 1m 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=81, pipe_handle=106)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \u001b[35m\"/Users/kaicho/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/__init__.py\"\u001b[0m, line \u001b[35m54\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from torch._utils_internal import (\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "  File \u001b[35m\"/Users/kaicho/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/_utils_internal.py\"\u001b[0m, line \u001b[35m11\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from torch._strobelight.compile_time_profiler import StrobelightCompileTimeProfiler\n",
      "  File \u001b[35m\"/Users/kaicho/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/_strobelight/compile_time_profiler.py\"\u001b[0m, line \u001b[35m3\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    import json\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/__init__.py\"\u001b[0m, line \u001b[35m106\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from .decoder import JSONDecoder, JSONDecodeError\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/decoder.py\"\u001b[0m, line \u001b[35m5\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from json import scanner\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/scanner.py\"\u001b[0m, line \u001b[35m11\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    NUMBER_RE = re.compile(\n",
      "        r'(-?(?:0|[1-9][0-9]*))(\\.[0-9]+)?([eE][-+]?[0-9]+)?',\n",
      "        (re.VERBOSE | re.MULTILINE | re.DOTALL))\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py\"\u001b[0m, line \u001b[35m289\u001b[0m, in \u001b[35mcompile\u001b[0m\n",
      "    return _compile(pattern, flags)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py\"\u001b[0m, line \u001b[35m350\u001b[0m, in \u001b[35m_compile\u001b[0m\n",
      "    p = _compiler.compile(pattern, flags)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/_compiler.py\"\u001b[0m, line \u001b[35m748\u001b[0m, in \u001b[35mcompile\u001b[0m\n",
      "    p = _parser.parse(p, flags)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/_parser.py\"\u001b[0m, line \u001b[35m980\u001b[0m, in \u001b[35mparse\u001b[0m\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/_parser.py\"\u001b[0m, line \u001b[35m459\u001b[0m, in \u001b[35m_parse_sub\u001b[0m\n",
      "    itemsappend(\u001b[31m_parse\u001b[0m\u001b[1;31m(source, state, verbose, nested + 1,\u001b[0m\n",
      "                \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                       \u001b[1;31mnot nested and not items)\u001b[0m)\n",
      "                       \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/_parser.py\"\u001b[0m, line \u001b[35m868\u001b[0m, in \u001b[35m_parse\u001b[0m\n",
      "    \u001b[31mstate.closegroup\u001b[0m\u001b[1;31m(group, p)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/_parser.py\"\u001b[0m, line \u001b[35m98\u001b[0m, in \u001b[35mclosegroup\u001b[0m\n",
      "    self.groupwidths[gid] = \u001b[31mp.getwidth\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "                            \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/_parser.py\"\u001b[0m, line \u001b[35m206\u001b[0m, in \u001b[35mgetwidth\u001b[0m\n",
      "    \u001b[1;31mhi\u001b[0m = hi + j * av[1]\n",
      "    \u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=81, pipe_handle=99)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \u001b[35m\"/Users/kaicho/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/__init__.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1;31mfrom torch._C import *\u001b[0m  # noqa: F403\n",
      "    \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m463\u001b[0m, in \u001b[35m_lock_unlock_module\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DE | mnist config evaluation] 21.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m full_results = \u001b[43mrun_full_benchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEXPERIMENT_PLAN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpatience\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_steps_per_epoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m full_results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mrun_full_benchmark\u001b[39m\u001b[34m(plan, training_override, download)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_override:\n\u001b[32m     51\u001b[39m     TRAINING_CONFIG.update(training_override)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mrun_experiment_suite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mEXPERIMENT_PLAN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m table = build_reporting_table()\n\u001b[32m     54\u001b[39m display(table)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mrun_experiment_suite\u001b[39m\u001b[34m(plan)\u001b[39m\n\u001b[32m     78\u001b[39m de_params = config.get(\u001b[33m\"\u001b[39m\u001b[33mde\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m de_params:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mrun_de\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mde_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m pso_params = config.get(\u001b[33m\"\u001b[39m\u001b[33mpso\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pso_params:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mrun_de\u001b[39m\u001b[34m(dataset_name, popsize, max_iter, mutation, recombination, training_config)\u001b[39m\n\u001b[32m     44\u001b[39m     history_rows.append(\n\u001b[32m     45\u001b[39m         {\n\u001b[32m     46\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(history_rows) + \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         }\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m result = \u001b[43mdifferential_evolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest1bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpopsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpopsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmutation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmutation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGLOBAL_SEEDS\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdating\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeferred\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolish\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m best_config = decode_vector(result.x)\n\u001b[32m     69\u001b[39m summary = evaluate_config(dataset_name, best_config, method=\u001b[33m\"\u001b[39m\u001b[33mDE\u001b[39m\u001b[33m\"\u001b[39m, training_config=training_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/scipy/_lib/_util.py:352\u001b[39m, in \u001b[36m_transition_to_rng.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    345\u001b[39m     message = (\n\u001b[32m    346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe NumPy global RNG was seeded by calling \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    347\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`np.random.seed`. Beginning in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunction will no longer use the global RNG.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    349\u001b[39m     ) + cmn_msg\n\u001b[32m    350\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/scipy/optimize/_differentialevolution.py:502\u001b[39m, in \u001b[36mdifferential_evolution\u001b[39m\u001b[34m(func, bounds, args, strategy, maxiter, popsize, tol, mutation, recombination, rng, callback, disp, polish, init, atol, updating, workers, constraints, x0, integrality, vectorized)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# using a context manager means that any created Pool objects are\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# cleared up.\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m DifferentialEvolutionSolver(func, bounds, args=args,\n\u001b[32m    488\u001b[39m                                  strategy=strategy,\n\u001b[32m    489\u001b[39m                                  maxiter=maxiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    500\u001b[39m                                  integrality=integrality,\n\u001b[32m    501\u001b[39m                                  vectorized=vectorized) \u001b[38;5;28;01mas\u001b[39;00m solver:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     ret = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/scipy/optimize/_differentialevolution.py:1186\u001b[39m, in \u001b[36mDifferentialEvolutionSolver.solve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.maxiter + \u001b[32m1\u001b[39m):\n\u001b[32m   1184\u001b[39m     \u001b[38;5;66;03m# evolve the population by a generation\u001b[39;00m\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1188\u001b[39m         warning_flag = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/scipy/optimize/_differentialevolution.py:1642\u001b[39m, in \u001b[36mDifferentialEvolutionSolver.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1639\u001b[39m trial_energies = np.full(\u001b[38;5;28mself\u001b[39m.num_population_members, np.inf)\n\u001b[32m   1641\u001b[39m \u001b[38;5;66;03m# only calculate for feasible entries\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1642\u001b[39m trial_energies[feasible] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_population_energies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrial_pop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeasible\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[38;5;66;03m# which solutions are 'improved'?\u001b[39;00m\n\u001b[32m   1646\u001b[39m loc = [\u001b[38;5;28mself\u001b[39m._accept_trial(*val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m   1647\u001b[39m        \u001b[38;5;28mzip\u001b[39m(trial_energies, feasible, cv, \u001b[38;5;28mself\u001b[39m.population_energies,\n\u001b[32m   1648\u001b[39m            \u001b[38;5;28mself\u001b[39m.feasible, \u001b[38;5;28mself\u001b[39m.constraint_violation)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/scipy/optimize/_differentialevolution.py:1338\u001b[39m, in \u001b[36mDifferentialEvolutionSolver._calculate_population_energies\u001b[39m\u001b[34m(self, population)\u001b[39m\n\u001b[32m   1336\u001b[39m parameters_pop = \u001b[38;5;28mself\u001b[39m._scale_parameters(population)\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     calc_energies = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mapwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters_pop\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1341\u001b[39m     calc_energies = np.squeeze(calc_energies)\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1343\u001b[39m     \u001b[38;5;66;03m# wrong number of arguments for _mapwrapper\u001b[39;00m\n\u001b[32m   1344\u001b[39m     \u001b[38;5;66;03m# or wrong length returned from the mapper\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/scipy/_lib/_util.py:575\u001b[39m, in \u001b[36m_FunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mrun_de.<locals>.objective\u001b[39m\u001b[34m(vector)\u001b[39m\n\u001b[32m     21\u001b[39m key = config_to_key(dataset_name, config)\n\u001b[32m     22\u001b[39m was_cached = key \u001b[38;5;129;01min\u001b[39;00m EVALUATION_CACHE\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m summary = \u001b[43mevaluate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m was_cached:\n\u001b[32m     25\u001b[39m     trial_counter[\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mevaluate_config\u001b[39m\u001b[34m(dataset_name, config, method, training_config)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Timer(label=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m config evaluation\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m GLOBAL_SEEDS:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         seed_result = \u001b[43mtrain_single_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m         seed_results.append(seed_result)\n\u001b[32m    173\u001b[39m total_runtime = time.time() - wall_start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mtrain_single_seed\u001b[39m\u001b[34m(dataset_name, config, seed, device, training_config)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    136\u001b[39m     model.load_state_dict(best_state)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m final_val_loss, final_val_acc = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m final_test_loss, final_test_acc = evaluate_model(model, test_loader, device)\n\u001b[32m    140\u001b[39m external_loader = build_external_loader(dataset_name, batch_size)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, loader, device)\u001b[39m\n\u001b[32m     37\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/comp815/Project_Report/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1172\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1165\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1171\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1172\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1174\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:58\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     55\u001b[39m cmd = spawn.get_command_line(tracker_fd=tracker_fd,\n\u001b[32m     56\u001b[39m                              pipe_handle=child_r)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m._fds.extend([child_r, child_w])\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawnv_passfds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_executable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/util.py:445\u001b[39m, in \u001b[36mspawnv_passfds\u001b[39m\u001b[34m(path, args, passfds)\u001b[39m\n\u001b[32m    443\u001b[39m errpipe_read, errpipe_write = os.pipe()\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_posixsubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfork_exec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassfds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrpipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrpipe_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_USE_VFORK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    451\u001b[39m     os.close(errpipe_read)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "full_results = run_full_benchmark(\n",
    "    plan=EXPERIMENT_PLAN,\n",
    "    training_override={\"epochs\": 3, \"patience\": 1, \"max_steps_per_epoch\": 150},\n",
    "    download=False,\n",
    " )\n",
    "full_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
